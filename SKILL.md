# SKILL.md — LLM Router as an agent tool

This file describes the **LLM Router** as a consumable skill for orchestrator agents. Use it as a reference when registering this tool in systems such as LangChain, Semantic Kernel, LlamaIndex, or any other orchestrator that calls tools via HTTP.

---

## Skill description

**Name:** `llm_router`
**Type:** HTTP tool (REST/JSON)
**Base URL:** `http://localhost:3000` (configurable)

Routes a prompt to the most suitable LLM model by applying:

- Semantic classification of the prompt (no API cost)
- Model selection by strategy (cost / quality / balanced)
- Automatic resilience via circuit breaker and retry
- Privacy control by sensitivity level

The agent does not need to know which model will be used — the router decides and returns the already-processed result.

---

## Available endpoints

### `POST /complete` — Execute a prompt

Sends a prompt and receives a response from the most suitable model.

**Request:**

```json
{
    "prompt": "string (required, minimum 1 char)",
    "options": {
        "strategy": "balanced | cost_first | quality_first",
        "sensitivity": "public | internal | sensitive",
        "forceCategory": "simple | code | reasoning | data_analysis | creative",
        "forceModel": "string (model ID on OpenRouter)",
        "maxCostPer1MTokens": "number (USD)",
        "requireContextWindow": "number (minimum tokens)"
    }
}
```

**Response:**

```json
{
    "content": "string — text generated by the model",
    "model": "string — ID of the model that responded",
    "category": "simple | code | reasoning | data_analysis | creative",
    "estimatedCostUsd": "number — estimated cost in USD",
    "latencyMs": "number — latency of the model call",
    "usage": {
        "inputTokens": "number",
        "outputTokens": "number"
    },
    "fallbackUsed": "boolean — true if the primary model failed and a fallback was used"
}
```

---

### `POST /feedback` — Correct a classification

Sends a correction to train the classifier. Use when the category in the response is wrong.

**Request:**

```json
{
    "prompt": "string",
    "correctCategory": "simple | code | reasoning | data_analysis | creative"
}
```

**Response:**

```json
{ "ok": true }
```

---

### `GET /health` — Check availability

**Response:**

```json
{
    "status": "ok",
    "model": "Xenova/multilingual-e5-small"
}
```

---

## Decision guide for the agent

### Which `strategy` to choose?

| Situation                            | Recommended `strategy` |
| ------------------------------------ | ---------------------- |
| Routine tasks, cost matters          | `cost_first`           |
| High quality is critical             | `quality_first`        |
| General use with no clear preference | `balanced` (default)   |

### Which `sensitivity` to use?

| Data in the prompt                  | `sensitivity` |
| ----------------------------------- | ------------- |
| Public data, no PII                 | `public`      |
| Internal/corporate data (default)   | `internal`    |
| PII, confidential or financial data | `sensitive`   |

> `sensitive` blocks models from providers that do not offer privacy guarantees (e.g. DeepSeek). Never omit this parameter when the prompt contains real user data.

### When to use `forceCategory`?

Use when you, as an agent, already know the task type and want to avoid semantic classifier latency or guarantee deterministic behaviour.

```jsonc
// Always use the best code model, regardless of the prompt
{ "forceCategory": "code" }
```

### When to use `forceModel`?

Use when the task requires a specific model for capability, cost, or compliance reasons.

```jsonc
// Force Claude for sensitive legal reasoning
{ "forceModel": "anthropic/claude-sonnet-4-5", "sensitivity": "sensitive" }
```

### When to use `maxCostPer1MTokens`?

Use to impose a hard budget per task. Models above the limit are excluded before ranking.

```jsonc
// Never spend more than $0.20/1M tokens on this task
{ "maxCostPer1MTokens": 0.2 }
```

---

## Task categories

| Category        | Example prompts                                                              |
| --------------- | ---------------------------------------------------------------------------- |
| `simple`        | Translate this text; Summarise in 3 points; What is X?                       |
| `code`          | Implement X in TypeScript; Refactor this function; Why is there a bug here?  |
| `reasoning`     | What are the tradeoffs of A vs B? How to architect X? Decide between Y and Z |
| `data_analysis` | Analyse this JSON; Write a SQL query for X; Interpret this log               |
| `creative`      | Write a README for X; Create an onboarding email; Generate marketing copy    |

---

## Call examples

### Code task with budget

```bash
curl -X POST http://localhost:3000/complete \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Implement a debounce in TypeScript with generic types",
    "options": {
      "strategy": "balanced",
      "sensitivity": "internal",
      "maxCostPer1MTokens": 0.5
    }
  }'
```

### Sensitive data analysis

```bash
curl -X POST http://localhost:3000/complete \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Analyse this transaction dataset and identify anomalies: [...]",
    "options": {
      "strategy": "quality_first",
      "sensitivity": "sensitive",
      "requireContextWindow": 100000
    }
  }'
```

### Simple task at minimum cost

```bash
curl -X POST http://localhost:3000/complete \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Translate to Spanish: Hello, how are you?",
    "options": {
      "strategy": "cost_first",
      "forceCategory": "simple"
    }
  }'
```

---

## Resilience behaviour

The router automatically applies:

- **Retry with exponential backoff**: up to 2 attempts per model before advancing in the fallback chain. Delays: 300ms, 600ms…
- **Circuit Breaker per model**: after 3 consecutive failures, the model is excluded for 60s. Automatic recovery via HALF_OPEN state.
- **Fallback chain**: if the primary model fails after retries, the next candidate is tried. `fallbackUsed: true` in the response indicates this.

If **all** models fail, the router returns HTTP 500.

---

## Integration as an agent tool (pseudo-schema)

```json
{
    "name": "llm_router_complete",
    "description": "Executes a prompt on the most suitable LLM model, automatically selected by cost, quality and data sensitivity. Returns the generated text, the model used, and cost metadata.",
    "parameters": {
        "prompt": {
            "type": "string",
            "description": "The prompt to send to the model"
        },
        "strategy": {
            "type": "string",
            "enum": ["balanced", "cost_first", "quality_first"],
            "description": "Model selection criterion",
            "default": "balanced"
        },
        "sensitivity": {
            "type": "string",
            "enum": ["public", "internal", "sensitive"],
            "description": "Sensitivity level of the data in the prompt",
            "default": "internal"
        }
    },
    "required": ["prompt"]
}
```
